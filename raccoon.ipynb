{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "loading and dividing the datsets"
      ],
      "metadata": {
        "id": "ylzQIJDvs6GB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vHJ-aTbHswPO",
        "outputId": "48e8a9bc-072e-48c6-b8c8-6dec2a80ed02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-0f9e26e9b2ce>:19: DtypeWarning: Columns (26) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df3= pd.read_csv(file3)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import os\n",
        "file1 = '/content/drive/MyDrive/dataset/Lumpy skin disease data.csv'\n",
        "file2 = '/content/drive/MyDrive/dataset/income.csv'\n",
        "file3 = '/content/drive/MyDrive/dataset/score.csv'\n",
        "file4 = '/content/drive/MyDrive/dataset/smoking.csv'\n",
        "df1 = pd.read_csv(file1)\n",
        "df2 = pd.read_csv(file2)\n",
        "df3= pd.read_csv(file3)\n",
        "df4 = pd.read_csv(file4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "data preprocessing"
      ],
      "metadata": {
        "id": "fvYHCBt0yHJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(df, targetcol):\n",
        "\n",
        "    x = df.drop(columns=[targetcol])\n",
        "    y = df[targetcol]\n",
        "\n",
        "    if y.dtypes == 'object' or y.dtypes.name == 'category':\n",
        "        new = LabelEncoder()\n",
        "        y = new.fit_transform(y)\n",
        "\n",
        "    x_temp, x_test, y_temp, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Further split train+val into train and val sets (75%-25%)\n",
        "    x_train, x_val, y_train, y_val = train_test_split(x_temp, y_temp, test_size=0.25, random_state=42)\n",
        "    scaler = StandardScaler()\n",
        "    x_train = scaler.fit_transform(x_train)\n",
        "    x_val = scaler.transform(x_val)\n",
        "    x_test = scaler.transform(x_test)\n",
        "\n",
        "    subsets = [{\"x\":x_train, \"y\":y_train}, {\"x\":x_val, \"y\":y_val}, {\"x\":x_test, \"y\":y_test}]\n",
        "    return subsets\n"
      ],
      "metadata": {
        "id": "hFySyHhtycTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "pytorch/tensorflow model and its training on a single node"
      ],
      "metadata": {
        "id": "_tMP9TZTJNqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLmodel(nn.Module):\n",
        "    def __init__(self, input_dim,hidden_dim, output_dim):\n",
        "        super(MLmodel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "def train_localmodel(nodeid,subset,input_dim,output_dim,epochs=50,lr=0.001,hidden_dim=64):\n",
        "  model = MLmodel(input_dim,hidden_dim, output_dim)\n",
        "  criteria = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "  x_tensor = torch.tensor(subset[\"x\"], dtype=torch.float32)\n",
        "  y_tensor = torch.tensor(subset[\"y\"], dtype=torch.long)\n",
        "\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    ans = model(x_tensor)\n",
        "    loss = criteria(ans,y_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  with torch.no_grad():\n",
        "        model.eval()\n",
        "  return model"
      ],
      "metadata": {
        "id": "yB8I1PXXKO4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "feaderated averaging and global model evaluation"
      ],
      "metadata": {
        "id": "Ap0DS1sZuZOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def aggregate_weights(localset):\n",
        "\n",
        "    globalset= {}\n",
        "    for key in localset[0].keys():\n",
        "      globalset[key] = sum(d[key] for d in localset) / len(localset)\n",
        "    return globalset\n",
        "\n",
        "def evaluate_model(model, subset):\n",
        "    model.eval()\n",
        "    x_tensor = torch.tensor(subset[\"x\"], dtype=torch.float32)\n",
        "    y_true = subset[\"y\"]\n",
        "    with torch.no_grad():\n",
        "        results = model(x_tensor)\n",
        "    _,y_pred = torch.max(results, 1)\n",
        "    acc = accuracy_score(y_true, y_pred.np())\n",
        "    return acc"
      ],
      "metadata": {
        "id": "HsPBx5l0uj2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "decentralised training"
      ],
      "metadata": {
        "id": "QBJEJZ-lzutd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "    # Choose your dataset and target column\n",
        "dfs = [df1, df2, df3, df4]\n",
        "target_columns = ['lumpy', 'income_>50K', 'Credit_Score', 'smoking']\n",
        "\n",
        "def train_and_evaluate(chosen_idx):\n",
        "    df = dfs[chosen_idx]\n",
        "    targetcol = target_columns[chosen_idx]\n",
        "\n",
        "    # ------- Centralized Training (whole dataset) -------\n",
        "    central_subsets = preprocess_data(df, targetcol)\n",
        "    input_dim = central_subsets[0][\"x\"].shape[1]\n",
        "    output_dim = len(np.unique(central_subsets[0][\"y\"]))\n",
        "    central_model = train_localmodel(0, central_subsets[0], input_dim, output_dim, epochs=50, lr=0.001, hidden_dim=64)\n",
        "    central_acc = evaluate_model(central_model, central_subsets[2])\n",
        "\n",
        "    global_models = []\n",
        "    global_weights = []\n",
        "\n",
        "# 1. Initialize a global model for each dataset\n",
        "    for df, targetcol in zip(dfs, target_columns):\n",
        "      input_dim = df.drop(columns=[targetcol]).shape[1]\n",
        "      output_dim = len(np.unique(df[targetcol]))\n",
        "      model = MLmodel(input_dim, 64, output_dim)\n",
        "      global_models.append(model)\n",
        "      global_weights.append(model.state_dict())\n",
        "\n",
        "    # ------- Federated/Decentralized Training (3 subsets) -------\n",
        "    idxs = np.random.permutation(len(df))\n",
        "    split_idxs = np.array_split(idxs, 3)\n",
        "    local_models = []\n",
        "    test_x, test_y = [], []\n",
        "\n",
        "    for i, idx_set in enumerate(split_idxs):\n",
        "        df_sub = df.iloc[idx_set].reset_index(drop=True)\n",
        "        subsets = preprocess_data(df_sub, targetcol)\n",
        "        input_dim = subsets[0][\"x\"].shape[1]\n",
        "        output_dim = len(np.unique(subsets[0][\"y\"]))\n",
        "\n",
        "        local_model = MLmodel(input_dim, 64, output_dim)\n",
        "        local_model.load_state_dict(global_weights[chosen_idx])\n",
        "\n",
        "        # Start from scratch for local models\n",
        "        trained_local_model = train_localmodel(\n",
        "            i, subsets[0], input_dim, output_dim, epochs=50, lr=0.001, hidden_dim=64\n",
        "        )\n",
        "        local_models.append(trained_local_model.state_dict())\n",
        "        test_x.append(subsets[2][\"x\"])\n",
        "        test_y.append(subsets[2][\"y\"])\n",
        "\n",
        "    # --- Aggregate local models for this dataset ---\n",
        "    agg_weights = aggregate_weights(local_models)\n",
        "    global_models[chosen_idx].load_state_dict(agg_weights)\n",
        "    global_weights[chosen_idx] = agg_weights\n",
        "\n",
        "    # --- Evaluate the updated global model for this dataset ---\n",
        "    x_test: np.ndarray  = np.concatenate(test_x)\n",
        "    y_test: np.ndarray = np.concatenate(test_y)\n",
        "    fed_test_acc = evaluate_model(fed_model, {\"x\": x_test, \"y\": y_test})\n",
        "\n",
        "    # Return metrics and optionally models if needed\n",
        "    return {\n",
        "        \"centralized_accuracy\": float(central_acc),\n",
        "        \"federated_accuracy\": float(fed_test_acc)\n",
        "        # Optionally, add more outputs (model weights, etc.)\n",
        "    }"
      ],
      "metadata": {
        "id": "-COkzHJOz6wp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gu8bIhkVXGgM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}